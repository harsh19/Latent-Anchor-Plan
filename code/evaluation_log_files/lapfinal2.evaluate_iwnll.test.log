args =  Namespace(bptt=5000, checkpoint='tmp/models/lapfinal2.pt', completion_mode='latent_var_model', conditional_data='', cuda=True, debug_mode=False, dedup=False, emotion_special_processing=False, emotion_type='basic', fake_endings='', happy_endings='', infer_nw_ignore_token_type='non_content_set2', inference_nw_kl_num_samples=1, inference_nw_uniform_distribution=False, inference_temperature=1.0, inference_use_argmax=False, iwnll_num_samples=5, keywords='', latent_plot_typ='kw', log_interval=100, model='LSTM', new_decoder=False, new_posterior_batch_defn=True, new_posterior_batch_defn_notitle=False, num_sentences_cond=4, outf='generated.txt', pretrain_ae_mode=False, previous_word_prior_dedup=False, print_cond_data=False, rake_as_inference=False, rake_model=False, sad_endings='', sanity_check_title=False, sanity_check_title_joined=False, sanity_check_unconditional=False, seed=1111, sentence_dedup=False, sents=40, story_body='', task='evaluate_iwnll', temperature=1.0, test_data='rocstory_plan_write/ROCStories_all_merge_tokenize.titlesepkeysepstory.test', top_p=0.0, true_endings='', use_argmax=False, use_emotion_supervision=False, vocab='tmp/vocabs/vocab.pkl', words=1000)
criterion =  CrossEntropyLoss()
[CORPUS]: Loading dictionary from provided path :  tmp/vocabs/vocab.pkl
[dictionary]: len(dictionary.word2idx) =  37905
*** skip_cnt =  0
*** not_found_cnt =  0
=========>>>> data : num batches  981
global_ignore_tokens =  [0, 19, 15, 82, 179, 9, 117, 327, 118, 1739, 133, 1308, 10214, 14963, 852, 2197, 26486, 2052, 86, 241, 26, 1729, 51, 56, 346, 546, 27, 1543, 7219, 33, 393, 273, 9551, 35, 126, 400, 354, 3174, 412, 151, 932, 684, 3127, 281, 1037, 20, 18, 614, 144, 111, 54, 819, 47, 1433, 794, 1950, 52, 463, 31, 310, 23, 29, 183, 132, 1899, 345, 21, 1461, 173, 55, 95, 468, 78, 124, 102, 454, 2545, 61, 859, 254, 1354, 80, 2094, 6230, 30, 130, 99, 110, 168, 136, 36, 469, 81, 971, 260, 4392, 568, 930, 3093, 309, 177, 149, 950, 49, 416, 994, 714, 455, 754, 160, 2005, 456, 198, 2580, 473, 7292, 157, 1281, 57, 210, 240, 657, 186, 604, 18235, 14526, 134, 1116, 48, 5805, 624, 297]
ignore_type_tokens_lists[args.infer_nw_ignore_token_type] =  ['<pad>', '.', '</s>', ',', '!', '<EOT>', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']
eval: total_loss =  tensor(1576.2280, device='cuda:0') eval: total_valid_tokens_cnt =  504 eval: total_loss/total_valid_tokens_cnt =  tensor(3.1274, device='cuda:0')
eval: total_loss =  tensor(173452.4375, device='cuda:0') eval: total_valid_tokens_cnt =  55943 eval: total_loss/total_valid_tokens_cnt =  tensor(3.1005, device='cuda:0')
eval: total_loss =  tensor(344894.2812, device='cuda:0') eval: total_valid_tokens_cnt =  111070 eval: total_loss/total_valid_tokens_cnt =  tensor(3.1052, device='cuda:0')
eval: total_loss =  tensor(514741.3438, device='cuda:0') eval: total_valid_tokens_cnt =  166014 eval: total_loss/total_valid_tokens_cnt =  tensor(3.1006, device='cuda:0')
eval: total_loss =  tensor(682652.5625, device='cuda:0') eval: total_valid_tokens_cnt =  220275 eval: total_loss/total_valid_tokens_cnt =  tensor(3.0991, device='cuda:0')
eval: total_loss =  tensor(853246.6250, device='cuda:0') eval: total_valid_tokens_cnt =  274562 eval: total_loss/total_valid_tokens_cnt =  tensor(3.1077, device='cuda:0')
eval: total_loss =  tensor(1023383.0625, device='cuda:0') eval: total_valid_tokens_cnt =  329138 eval: total_loss/total_valid_tokens_cnt =  tensor(3.1093, device='cuda:0')
eval: total_loss =  tensor(1188074.6250, device='cuda:0') eval: total_valid_tokens_cnt =  382634 eval: total_loss/total_valid_tokens_cnt =  tensor(3.1050, device='cuda:0')
eval: total_loss =  tensor(1344835.8750, device='cuda:0') eval: total_valid_tokens_cnt =  435312 eval: total_loss/total_valid_tokens_cnt =  tensor(3.0894, device='cuda:0')
eval: total_loss =  tensor(1503693.2500, device='cuda:0') eval: total_valid_tokens_cnt =  488269 eval: total_loss/total_valid_tokens_cnt =  tensor(3.0796, device='cuda:0')
EVAL: total_loss =  tensor(1627488.3750, device='cuda:0')
EVAL: total_valid_tokens_cnt =  530052
=========================================================================================
| test_eval | test loss  3.07 | test ppl    21.55 | test bpc    4.430
=========================================================================================
